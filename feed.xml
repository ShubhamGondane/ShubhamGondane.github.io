<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shubhamgondane.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shubhamgondane.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-03T21:18:58+00:00</updated><id>https://shubhamgondane.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">The Dilemma of Progress</title><link href="https://shubhamgondane.github.io/blog/2025/the-coming-wave/" rel="alternate" type="text/html" title="The Dilemma of Progress"/><published>2025-03-02T06:00:00+00:00</published><updated>2025-03-02T06:00:00+00:00</updated><id>https://shubhamgondane.github.io/blog/2025/the-coming-wave</id><content type="html" xml:base="https://shubhamgondane.github.io/blog/2025/the-coming-wave/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03_the_coming_wave/the_coming_wave_book-480.webp 480w,/assets/img/2025-02-03_the_coming_wave/the_coming_wave_book-800.webp 800w,/assets/img/2025-02-03_the_coming_wave/the_coming_wave_book-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-02-03_the_coming_wave/the_coming_wave_book.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="The Coming Wave" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <a href="https://a.co/d/4Qwlzob">The Coming Wave by Mustafa Suleyman and Michael Bhaskar</a> </div> <p>I recently finished <strong>The Coming Wave</strong> by Mustafa Suleyman and Michael Bhaskar, and I’d like to share some of my thoughts, particularly on the topic of AI. The book explores the four transformative forces of the coming wave: <strong>Artificial Intelligence, Synthetic Biology, Quantum Computing</strong>, and emerging energy sources like <strong>Nuclear Fusion</strong>. AI and quantum computing are dominating the conversation these days, with new LLM models being launched almost weekly and major tech giants like Amazon, Microsoft, and Google unveiling groundbreaking research in quantum computing.</p> <p>So far, we’ve experienced 24 major waves of innovation. Over a span of 10,000 years up until 1000 BCE, 7 general-purpose technologies emerged. Between 1700 and 1900, there were 6 waves, ranging from the steam engine to electricity. In the past 100 years alone, we’ve seen 7 new waves, from airplanes to nuclear power.</p> <h3 id="the-characteristics-of-the-coming-wave-technologies">The characteristics of the coming wave technologies</h3> <h4 id="1-new-technology-has-a-potential-to-scale-assymetrically">1. New technology has a potential to scale assymetrically.</h4> <ul> <li>1 Tweet -&gt; Can spread an idea globally in an instant.</li> <li>1 Startup -&gt; Just one breakthrough algorithm can build a massive global company.</li> <li>1 AI code -&gt; Has the potential to write more code than all of humanity combined.</li> <li>1 Bio experiment -&gt; Could unleash a new pathogen, sparking a pandemic.</li> <li>1 Viable quantum computer -&gt; Has the power to render all encryption protocols obsolete.</li> </ul> <h4 id="2-hyper-evolution">2. Hyper Evolution</h4> <p>These technologies are evolving rapidly and its difficult to contain these and their impact is global.</p> <h4 id="3-omni-use">3. Omni Use</h4> <p>These technologies have a broad spectrum of use cases, regardless of their original intentions and overtime the uses tend towards generality.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03_the_coming_wave/Use_case_spectrum-480.webp 480w,/assets/img/2025-02-03_the_coming_wave/Use_case_spectrum-800.webp 800w,/assets/img/2025-02-03_the_coming_wave/Use_case_spectrum-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-02-03_the_coming_wave/Use_case_spectrum.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="The Use Case Spectrum" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="4-autonomy">4. Autonomy</h4> <p>Every iteration of these technologies make them more autonomous.</p> <h3 id="unstoppable-incentives">Unstoppable incentives</h3> <p>There are unstoppable incentives to continue on the development of these technologies. Lets take the example of AI.</p> <h4 id="1-geopolitics">1. Geopolitics</h4> <p>Nation states need to innovovate if they want to stay on top. AI has become a pivotal factor in global geopolitics, influencing economic strategies, national security policies, and international relations. Major powers like the United States, China, the European Union (EU), and emerging players such as India are heavily investing in AI to secure technological leadership and economic growth. The <a href="https://techcrunch.com/2025/01/21/openai-teams-up-with-softbank-and-oracle-on-50b-data-center-project/">Stargate Project</a>, a joint venture between OpenAI, Oracle, SoftBank, and other tech companies, with support from the US government plans to invest up to $500 billion over the next 4 years. Not just nation states, BigTech companies also need to innovate to stay on top and they have plan to spend more than $300 billion in 2025 on AI technologies and datacenter solutions. Not to be outdone by US and China, <a href="https://www.wsj.com/tech/ai/eu-pledges-200-billion-in-ai-spending-in-bid-to-catch-up-with-u-s-china-7bf82ab5">EU has plans to invest </a> 200 billion euros and India has plans <a href="https://www.reuters.com/technology/india-announces-12-bln-investment-ai-projects-2024-03-07/">to invest $1.25 billion</a> to develop computing infrastructure and for the development of large language models.</p> <h4 id="2-open-source-research">2. Open source research</h4> <p>Meta has been the leading force in open-source AI, with their Llama models driving research and innovation. However, the landscape is shifting. Deepseek-AI’s recent emergence has shaken the AI world, triggering significant market volatility, including a <a href="https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/">dramatic decline in Nvidia’s market capitalization</a>. The findings from Deepseek-AI’s DeepSeek-R1 release and accompanying <a href="https://arxiv.org/abs/2501.12948">methodology</a> are undoubtedly being explored by leading AI companies.</p> <h4 id="3-financial-gains">3. Financial gains</h4> <p>Pinpointing the exact economic gains from AI is inherently difficult. However, a conservative projection by <a href="https://mitsloan.mit.edu/ideas-made-to-matter/a-new-look-economics-ai">MIT Professor Daron Acemoglu indicates a possible 1% growth in US GDP within the next decade</a>.</p> <h4 id="4-ego">4. Ego</h4> <p>The biggest example, that we are seeing play out in front of us the rivalry between Elon Musk and Sam Altman. Their feud stems from Musk’s belief that OpenAI betrayed its original mission by prioritizing profit and aligning too closely with Microsoft. With <strong>Musk backing xAI</strong> and <strong>Altman leading OpenAI</strong>, the competition between their respective companies could push for:</p> <ul> <li>Faster AI advancements as each side competes to develop more powerful models.</li> <li>Diverging AI philosophies—Musk advocates for <strong>“AI safety first”</strong>, while Altman pushes for <strong>“rapid innovation with safeguards”</strong>.</li> </ul> <h2 id="my-thoughts-on-ai">My thoughts on AI</h2> <h3 id="current-progress">Current progress</h3> <p>Evolution, a process of iterative refinement, ultimately led to humanity. Imagine it as nature testing countless variations, culminating in us. While our evolution continues, it’s now a slow crawl. The impact? Profound. Human intelligence has enabled us to manipulate natural resources and translate knowledge into powerful applications, dramatically reshaping our environment. This transformation, initially gradual, has accelerated exponentially in recent decades.</p> <p>The pursuit of Artificial General Intelligence (AGI) mirrors evolution’s iterative process, with numerous models being explored. While the precise impact of AGI remains uncertain, its potential magnitude, whether positive or negative, is unprecedented. Although timelines are debated, achieving something akin to AGI within the next few decades appears increasingly likely. Despite claims by some, like Sam Altman, current Large Language Models (LLMs) still lack the robust reasoning necessary for genuine world understanding.</p> <p>The gap between claims of approaching AGI and the demonstrable limitations of current LLMs is evident in examples like their <a href="https://www.reddit.com/r/OpenAI/comments/1haxhjk/can_someone_explain_exactly_why_llms_fail_at/">inability to accurately count letters</a> or <a href="https://www.reddit.com/r/singularity/comments/1e4fcxm/none_of_current_llms_can_truly_reason_and_cannot/">perform basic number comparisons</a>. Even the GOAT, Karpathy, highlights these limitations in his latest YouTube video - <a href="https:/www.youtube.com/watch?v=7xTGNNLPyMI&amp;t=7271s">Deep Dive into LLMs like ChatGPT</a>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03_the_coming_wave/strawberry_fail-480.webp 480w,/assets/img/2025-02-03_the_coming_wave/strawberry_fail-800.webp 800w,/assets/img/2025-02-03_the_coming_wave/strawberry_fail-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-02-03_the_coming_wave/strawberry_fail.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Strawberry LLM Fail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Even latest models such as Gemini 2.0 fail </div> <p>I feel that companies are incentivized to say AGI is here or close to raise funding or to justify the billions of $ spent in capital expenditure as they chase bigger and bigger models.</p> <h3 id="my-worries-about-ai">My worries about AI</h3> <p>If AI enables us to accomplish more with significantly less effort, the traditional need for large teams supporting a project may diminish. This shift raises a crucial question: will this efficiency lead to widespread unemployment, or will it instead fuel an explosion of new software products and innovations?</p> <p>AI’s ability to perform tasks previously handled by professionals also prompts an unsettling concern: what happens to the value of human knowledge? As AI can replicate—and in many cases, surpass—human skills, how can individuals ensure they remain indispensable? The challenge now goes beyond simply learning AI; it’s about positioning oneself to work alongside it rather than being replaced by it.</p> <p>Many economists argue that as technology replaces existing jobs, new ones will be created. But in the age of AI, as it continuously learns and improves, it may eventually take over these new roles as well. This raises an important issue: will the new jobs created by AI be enough to absorb the millions displaced? Not everyone will have the skills or resources to transition into these new roles.</p> <p>While jobs in the physical domain—such as cooks, masseuses, or other hands-on roles—may remain safer from automation, the real question is how many jobs in the information domain will remain immune to AI’s reach. As AI permeates more industries, it could make even information-based professions increasingly redundant, challenging our traditional understanding of work and human contribution.</p> <p>The future workforce may not just involve humans augmenting AI; it could very well involve humans finding new ways to coexist with machines in a landscape where the value of human labor is rapidly evolving. And so the real question isn’t just about job displacement—it’s about redefining work itself. <strong>Will the future workforce consist of humans augmenting AI, or will AI make human contribution increasingly redundant?</strong></p> <h3 id="genetic-engineering-vs-software-engineering">Genetic Engineering vs Software Engineering</h3> <p>The risks associated with genetic engineering are significantly higher than those in software engineering, primarily because biology operates in the real world, where unintended consequences can have profound and irreversible effects. A single mistake in genetic manipulation could lead to unforeseen mutations, ecological disruptions, or even biosecurity threats. As more individuals experiment with biohacking, the risks compound, making biological tinkering a potential global concern.</p> <p>Software engineering, on the other hand, primarily exists in the digital realm, where errors—though sometimes costly—are generally more contained. Bugs in code can be patched, systems can be rebooted, and security vulnerabilities can be mitigated. However, the rise of artificial intelligence changes this equation. AI-driven applications now have the power to influence financial markets, critical infrastructure, healthcare decisions, and even warfare. While traditional software errors might not directly impact human biology, AI-enhanced software can blur the line between digital and physical consequences, making certain failures just as dangerous as biological mishaps.</p> <p>One of the most striking developments in genetic AI is <a href="https://arcinstitute.org/news/blog/evo2">Evo2</a>, the latest LLM trained on <strong>9.3 billion DNA sequences</strong>. What makes Evo2 particularly alarming is its <strong>open-source nature</strong>, granting anyone—regardless of expertise—access to powerful genetic editing capabilities. In theory, this means that individuals with minimal formal training could modify DNA sequences from their own homes, synthesizing genetic changes in makeshift laboratories. The implications of such unrestricted access are vast, ranging from groundbreaking medical advancements to the potential for bioengineered pathogens.</p> <p>As AI continues to integrate into both software and genetic engineering, the line between these fields is becoming increasingly blurred. The real question is: <strong>Are we prepared for the consequences of democratizing such powerful technologies?</strong></p> <h2 id="the-containment-plan">The Containment Plan</h2> <p>Suleyman’s <strong>containment plan for the coming wave of technological challenges</strong> offers a thorough, multi-faceted strategy for managing potentially disruptive technologies such as AI and biotechnology. This is a highly complex subject that warrants a dedicated post of its own. This is in itself a very complex topic that deserves a separate post on its own. For now, the visual below offers a broad overview of the key elements of the containment plan, which will be explored in greater detail later.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03_the_coming_wave/The_Containment_Plan-480.webp 480w,/assets/img/2025-02-03_the_coming_wave/The_Containment_Plan-800.webp 800w,/assets/img/2025-02-03_the_coming_wave/The_Containment_Plan-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-02-03_the_coming_wave/The_Containment_Plan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="The Containment Plan" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Ultimately, <strong>The Coming Wave</strong> paints a stark, yet vital, picture of the technological future we’re hurtling towards. As Mustafa Suleyman argues, the characteristics of these powerful technologies, coupled with the relentless incentives driving their development, particularly evident in the rapid evolution of AI, present us with unprecedented challenges. While the potential benefits are undeniable, the risks—the erosion of control, the potential for misuse, and the sheer scale of societal disruption—cannot be ignored. <strong>The Containment Plan</strong> offers a framework for navigating this turbulent landscape, but its success hinges on a collective commitment to proactive governance, ethical development, and a fundamental shift in how we perceive and manage technological progress. <strong>Frankly, given the current geopolitical climate and the disparate motivations of those driving technological advancement, this level of unified, proactive action feels like a monumental, perhaps even impossible, undertaking</strong>. Yet, the wave is coming, and whether it washes over us or carries us forward depends entirely on the choices we make today. The conversation, and more importantly, the action, must begin now.</p> <h3 id="references">References</h3> <ul> <li> <p>Suleyman, M., &amp; Bhaskar, M. (2023). <em>The Coming Wave: Technology, Power, and the Twenty-first Century’s Greatest Dilemma</em>. Crown.</p> </li> <li><a href="https://techcrunch.com/2025/01/21/openai-teams-up-with-softbank-and-oracle-on-50b-data-center-project/">OpenAI teams up with SoftBank and Oracle on $50B data center project</a></li> <li><a href="https://www.wsj.com/tech/ai/eu-pledges-200-billion-in-ai-spending-in-bid-to-catch-up-with-u-s-china-7bf82ab5">EU Pledges $200 Billion in AI Spending in Bid to Catch Up With U.S., China</a></li> <li><a href="https://www.reuters.com/technology/india-announces-12-bln-investment-ai-projects-2024-03-07/">India announces $12 bln investment in AI projects</a></li> <li><a href="https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/">China’s DeepSeek sets off AI market rout</a></li> <li><a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li> <li><a href="https://mitsloan.mit.edu/ideas-made-to-matter/a-new-look-economics-ai">A New Look at the Economics of AI</a></li> <li><a href="https://www.youtube.com/watch?v=7xTGNNLPyMI&amp;t=7271s">Deep Dive into LLMs like ChatGPT</a></li> <li><a href="https://arcinstitute.org/news/blog/evo2">EVO2</a></li> </ul>]]></content><author><name></name></author><category term="book-reviews"/><category term="ai"/><summary type="html"><![CDATA[Thoughts on the book - The Coming Wave]]></summary></entry><entry><title type="html">From Siloed Systems to Unified Platforms: Exploring the Lakehouse Revolution</title><link href="https://shubhamgondane.github.io/blog/2025/from-siloed-systems-to-unified-platforms-exploring-the-lakehouse-revolution/" rel="alternate" type="text/html" title="From Siloed Systems to Unified Platforms: Exploring the Lakehouse Revolution"/><published>2025-02-10T16:10:16+00:00</published><updated>2025-02-10T16:10:16+00:00</updated><id>https://shubhamgondane.github.io/blog/2025/from-siloed-systems-to-unified-platforms-exploring-the-lakehouse-revolution</id><content type="html" xml:base="https://shubhamgondane.github.io/blog/2025/from-siloed-systems-to-unified-platforms-exploring-the-lakehouse-revolution/"><![CDATA[<h4>The Evolution of Data Platforms and the Rise of the Lakehouse</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SqBEpcLp2tto2t2c.jpeg"/><figcaption>Image generated by Gemini</figcaption></figure> <p>The explosion of data in recent years has presented both opportunities and challenges for businesses. While data lakes offered a flexible and scalable solution for storing vast amounts of raw data, they often lacked the data management capabilities and performance of traditional data warehouses. This disconnect created a significant hurdle for organizations seeking to leverage their data for both analytical insights and advanced applications like machine learning. The paper “Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics” introduced a compelling solution: the lakehouse. This architecture aims to combine the best of both worlds, offering the low-cost storage and schema-on-read flexibility of data lakes with the ACID transactions, data versioning, and query optimization of data warehouses. This article summarizes the key concepts presented in the paper, exploring the motivations behind the lakehouse architecture, its core components, and the challenges it addresses. Furthermore, we will examine how current lakehouse systems like Apache Hudi, Apache Iceberg, and Delta Lake are implementing these principles and shaping the future of data management.</p> <h3>History of data warehousing</h3> <p>Before diving into lakehouses, it’s important to understand the history and limitations of previous systems, as these shortcomings motivated the development of lakehouses.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/524/0*2NrCus6UzkoBvKdL.png"/><figcaption><strong>First and Second generation of data warehouse </strong>(Zaharia, M.A., Ghodsi, A., Xin, R., &amp; Armbrust, M. (2021). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. <em>Conference on Innovative Data Systems Research</em>.)</figcaption></figure> <h3>First Generation</h3> <p>Early data warehouses typically collected data from operational databases and centralized it for analytical insights. These warehouses used a schema-on-write approach, optimizing data models for downstream business intelligence. However, with the explosion of data generation, these systems encountered several problems. First, their coupled compute and storage meant that enterprises paid increasingly more for both as data volumes grew. Second, the rising data volume wasn’t just tabular data; it also included unstructured formats like audio, video, text, and images, which traditional warehouses couldn’t handle.</p> <h3>Second Generation</h3> <p>To address these issues, the second generation of data systems emerged, where enterprises dumped all raw data into a data lake. This data resided in generic, open file formats like Parquet and ORC, primarily supporting a schema-on-read approach. A pipeline would extract a subset of data from the lake and load it into a data warehouse for critical decision support and business intelligence applications. Data lakes were typically stored on cloud platforms like AWS S3, Azure Data Lake Storage (ADLS), and Google Cloud Storage (GCS), enabling the separation of storage and compute. Raw data resided in these storage systems and was then ETLed (Extract, Transform, Load) into downstream data warehouses such as BigQuery, Redshift, or Snowflake.</p> <p>The data lake and warehouse approach, while addressing some earlier challenges, introduced its own set of problems:</p> <ul><li><strong>Reliability:</strong> Ensuring data consistency, accuracy, completeness, and validity over time, regardless of source or transformations, proved difficult. Maintaining consistency between the data lake and the warehouse was challenging due to data quality issues, bugs, and the high cost of engineering required to make data available for business intelligence.</li><li><strong>Data Staleness:</strong> Data in the warehouse often became outdated, no longer reflecting the current state of affairs. A significant lag existed between updates to the data lake and corresponding updates to the warehouse.</li><li><strong>Limited Support for Advanced Analytics:</strong> Building machine learning systems on top of data warehouses was difficult. Analyzing large datasets with complex tools using traditional warehouse access methods was inefficient. Using raw data directly from the data lake was also problematic because the transformed features created and used in the warehouse were not available.</li><li><strong>Total Cost of Ownership:</strong> Enterprises incurred double the storage costs due to data being duplicated in both the data lake and the warehouse.</li></ul> <p>The lakehouse architecture attempts to address several of the challenges outlined above. However, data quality and reliability remain key concerns for users.</p> <h3>The lakehouse architecture</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/310/0*ojkqfBV2udkwyH92.png"/><figcaption><strong>Data Lakehouse</strong> (Zaharia, M.A., Ghodsi, A., Xin, R., &amp; Armbrust, M. (2021). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. <em>Conference on Innovative Data Systems Research</em>.)</figcaption></figure> <p>The authors define a lakehouse as a data management system built on low-cost, directly accessible storage that also provides the management and performance features of traditional analytical database management systems (DBMS), such as ACID transactions, data versioning, auditing, indexing, caching, and query optimization. It combines the best of both worlds: the low-cost, open-format storage of data lakes with the powerful management and optimization capabilities of data warehouses.</p> <p>The first step in implementing a lakehouse is to utilize a low-cost object store like S3 or GCS, using open standard file formats such as Parquet. Next, a metadata layer is implemented on top of the object store. This layer defines which objects belong to a specific table version, enabling features like ACID transactions. Several systems have adopted this lakehouse architecture, including Delta Lake, Apache Iceberg, and Apache Hudi.</p> <h4>Metadata Layer</h4> <p>Data lake storage systems like S3 or HDFS offer only a low-level object store or file system interface. Even simple operations, such as atomically updating a table that spans multiple files, are not supported. This is where a metadata layer becomes crucial. It stores information about which objects belong to a table within a transaction log, enabling ACID properties. The metadata layer also facilitates schema enforcement and allows setting constraints on ingested data.</p> <h4>SQL Performance</h4> <p>Lakehouses employ various techniques to improve SQL query performance:</p> <ul><li><strong>Caching:</strong> Lakehouses can store frequently accessed data in a faster storage layer (like memory or SSD) to accelerate retrieval times.</li><li><strong>Auxiliary Data:</strong> Storing data statistics, such as min-max values for a column, allows the lakehouse to perform data skipping during reads. Implementing indexes like Bloom filters further speeds up data retrieval.</li><li><strong>Data Layout:</strong> Record ordering, such as clustering, helps by grouping frequently accessed records together. All lakehouses support ordering records using individual dimensions or space-filling curves like Z-order and Hilbert curves to improve locality across multiple dimensions.</li></ul> <p>The authors have demonstrated the effectiveness of these optimizations. The graph below shows the execution time and the total cost for customers under each service’s pricing model.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/696/0*GzKQH5J0J6Z4-bWm.png"/><figcaption>Zaharia, M.A., Ghodsi, A., Xin, R., &amp; Armbrust, M. (2021). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. <em>Conference on Innovative Data Systems Research</em>.</figcaption></figure> <h3>Current State of Lakehouse Systems</h3> <p>The authors also discuss future directions and alternative designs, which we will explore below, comparing them to current open-table formats for lakehouses. We’ll focus on Apache Hudi, Apache Iceberg, and Delta Lake.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qe6HlopYsuJf1Cmt.png"/></figure> <h4>New Data Lake Storage Format</h4> <p>The authors suggest that a new storage format could provide more flexibility for lakehouse systems, enabling better data layout optimizations, indexing, and compatibility with modern hardware. While current table formats support open file formats like Parquet, ORC, and Avro, the emergence of a new, more optimized format remains a possibility.</p> <h4>Alternative Designs for Metadata Layers</h4> <p>Each lakehouse system employs a unique metadata management strategy:</p> <ul><li><strong>Apache Hudi:</strong> Uses a timeline-based approach, storing metadata in a dedicated HFile-backed metadata table with indexes for files, column statistics, Bloom filters, and record locations. This facilitates efficient transaction management and data skipping.</li><li><strong>Apache Iceberg:</strong> Employs a three-tier metadata tree structure (metadata.json, manifest lists, and manifests) designed for efficient handling of large-scale tables with numerous partitions and files. This hierarchical structure enables efficient metadata operations.</li><li><strong>Delta Lake:</strong> Combines transaction logs (JSON files recording each table change) and Parquet checkpoints (periodic snapshots of the table state) for metadata management. Metadata is stored in the Delta Log, enabling efficient change tracking, time travel, and ACID transactions.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2jlSNFadLS8GbjS3AUMnvQ.png"/><figcaption>Metadata Management Comparison</figcaption></figure> <h4>Different strategies for caching, auxiliary data structures, and data layout.</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Z84dazIuaXM3apTZ.png"/></figure> <p><strong>Optimizations: Caching, Auxiliary Data Structures, and Data Layout</strong></p> <p>All three systems utilize similar optimization strategies:</p> <ul><li><strong>Efficient Metadata Management:</strong> All three systems utilize metadata to avoid full table scans.</li><li><strong>Data Skipping:</strong> Metadata and statistics (e.g., column bounds, Bloom filters) are used to skip irrelevant data files during query execution.</li><li><strong>Indexing:</strong> While not always explicit, metadata structures act as indexes, enabling efficient data lookups and filtering.</li><li><strong>Caching:</strong> Frequently accessed metadata is cached to minimize I/O operations and improve query performance.</li></ul> <p>While the core principles are similar, each system implements these optimizations differently. For example, Iceberg emphasizes scan planning, metadata filtering, and predicate pushdown. Hudi focuses on secondary indexing, a dedicated metadata table, and incremental queries. Delta Lake leverages transaction logs and checkpoints for efficient metadata management and data skipping.</p> <p>In summary, Apache Iceberg, Apache Hudi, and Delta Lake are open-source table formats designed to enhance data lake performance and reliability through efficient metadata management, data skipping, indexing, and caching. Each system has its own implementation details and strengths.</p> <h4>ML integrations</h4> <p>All three lakehouse formats (Hudi, Iceberg, and Delta Lake) support time travel and snapshot data access, allowing engineers to retrieve historical data versions. This is valuable for auditing models or retraining them on past data states.</p> <p>Each format also integrates with various machine learning tools:</p> <ul><li><strong>Hudi:</strong> Works with Spark-based ML frameworks and integrates with AWS Glue and Amazon Athena.</li><li><strong>Iceberg:</strong> Integrates with AWS Glue and is supported by Amazon SageMaker Feature Store, enabling faster query performance for building training datasets.</li><li><strong>Delta Lake:</strong> Excels at unifying batch and streaming data processing for end-to-end ML workflows. Its tight integration with MLflow provides experiment tracking, model versioning, and reproducibility, simplifying the management of the entire ML lifecycle from data preparation to deployment.</li></ul> <h3>Conclusion</h3> <p>The lakehouse architecture represents a significant step forward in the evolution of data platforms. By unifying data warehousing and advanced analytics, it empowers organizations to unlock the full potential of their data. The paper “Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics” laid the foundation for this new paradigm, highlighting the key challenges and opportunities. As we’ve seen, open-source projects like Apache Hudi, Apache Iceberg, and Delta Lake are actively building upon these concepts, each with its own strengths and approaches. While challenges like data quality and governance still persist, the lakehouse is rapidly becoming the preferred architecture for organizations seeking to build a modern, scalable, and efficient data platform. The ability to seamlessly integrate diverse data types, perform complex analytics, and support advanced machine learning workloads within a single system promises to accelerate innovation and drive data-driven decision-making across industries. As the lakehouse ecosystem continues to mature, we can expect even greater advancements in performance, security, and ease of use, further solidifying its position as the future of data management.</p> <h3>References</h3> <ul><li>Zaharia, M.A., Ghodsi, A., Xin, R., &amp; Armbrust, M. (2021). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. <em>Conference on Innovative Data Systems Research</em>.</li><li><a href="https://www.databricks.com/research/lakehouse-a-new-generation-of-open-platforms-that-unify-data-warehousing-and-advanced-analytics">https://www.databricks.com/research/lakehouse-a-new-generation-of-open-platforms-that-unify-data-warehousing-and-advanced-analytics</a></li><li><a href="https://hudi.apache.org/docs/overview">https://hudi.apache.org/docs/overview</a></li><li><a href="https://iceberg.apache.org/docs/nightly/">https://iceberg.apache.org/docs/nightly/</a></li><li><a href="https://delta.io">https://delta.io</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7b590042e3fc" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://blog.det.life/from-siloed-systems-to-unified-platforms-exploring-the-lakehouse-revolution-7b590042e3fc">From Siloed Systems to Unified Platforms: Exploring the Lakehouse Revolution</a> was originally published in <a href="https://blog.det.life">Data Engineer Things</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Cost Optimization Part 2</title><link href="https://shubhamgondane.github.io/blog/2025/cost-optimization-part-2/" rel="alternate" type="text/html" title="Cost Optimization Part 2"/><published>2025-01-20T06:01:26+00:00</published><updated>2025-01-20T06:01:26+00:00</updated><id>https://shubhamgondane.github.io/blog/2025/cost-optimization-part-2</id><content type="html" xml:base="https://shubhamgondane.github.io/blog/2025/cost-optimization-part-2/"><![CDATA[<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg" width="728" height="728" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;normal&quot;,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:728,&quot;bytes&quot;:372437,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9def3797-326b-4217-a840-5a3c49ffdd3a_2048x2048.jpeg 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p>In the <a href="https://shubhamgondane.substack.com/p/data-engineering-cost-optimization?r=52p1v">previous post</a>, I covered a few general optimization techniques to reduce the cost of running your Spark pipelines. These techniques were simple enough to try for quick gains. This post focuses on advanced techniques and combinations to further optimize your pipeline.</p> <p></p> <div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://shubhamgondane.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading The Data Engineering Newsletter! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"/><input type="submit" class="button primary" value="Subscribe"/><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div> <h2>Compression</h2> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png" width="1389" height="1095" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1095,&quot;width&quot;:1389,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1022847,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83e16f1-4e26-4208-a526-ca6c02b6d649_1389x1095.png 1456w" sizes="100vw"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p>Data compression involves making data smaller without losing its essential information. In Spark, where we process large volumes of data, compression helps improve pipeline performance by:</p> <ul><li><p><strong>Reducing disk I/O and network usage:</strong> Compressed data takes up less space, leading to faster read/write operations and reduced data transfer times.</p></li><li><p><strong>Lowering storage costs:</strong> Smaller data footprints translate to lower storage costs, especially in cloud environments.</p></li></ul> <p>Spark offers various compression codecs, each with its own benefits and tradeoffs:</p> <ul><li><p><strong>Snappy:</strong> Developed by Google, Snappy prioritizes high speeds and reasonable compression over maximum compression ratios. It's a good general-purpose codec.</p></li><li><p><strong>Zstd (Zstandard):</strong> Developed by Facebook, Zstd provides high compression ratios and fast compression speeds. It even includes a "dictionary compression" mode for small data.</p></li><li><p><strong>LZ4:</strong> This lossless compression algorithm offers extremely fast compression and decompression speeds, often reaching RAM speed limits on multi-core systems.</p></li></ul> <p><strong>Enabling Compression in Spark:</strong></p> <p>The default compression codec in Spark is Snappy. To change it, for example to Zstd, you can set the compression option during write operations:</p> <p>PySpark</p> <pre><code><code>df.write.option("compression", "zstd").parquet("path/to/output") 
</code></code></pre> <p>This code snippet shows how to write the DataFrame to Parquet files at the specified path using Zstd compression.</p> <p>Selecting the optimal compression codec depends heavily on your data characteristics and specific use case. Snappy is a popular choice due to its speed and reasonable compression. However, if minimizing storage space is critical, consider Zstd, which offers higher compression ratios but may require slightly more processing time. Ultimately, the best approach is to experiment with different codecs and evaluate their performance based on your needs and priorities.</p> <p></p> <h2>Salting</h2> <p>Salting is a well-known technique for handling data skew in Apache Spark. Data skew occurs when data is unevenly distributed across partitions, leading to inefficient resource utilization, longer processing times, and increased costs.</p> <p>For example, imagine joining two Spark DataFrames where one has skewed data. This means a join column's values are concentrated in a few partitions, while others remain relatively empty. This imbalance forces a few partitions to do most of the work, creating bottlenecks.</p> <p>Here's how salting can help:</p> <ol><li><p><strong>Identify the skewed key:</strong> Determine the column causing the skew.</p></li><li><p><strong>Add a random salt to the key:</strong> Introduce random prefixes or suffixes to the skewed key values. This redistributes data more evenly across partitions.</p></li><li><p><strong>Repartition and join:</strong> Repartition both DataFrames using the salted key and perform the join.</p></li></ol> <p><strong>Example:</strong></p> <p>PySpark</p> <pre><code><code>df = df.withColumn('salted_col', concat(col('skewed_col'), lit('_'), rand())) 
</code></code></pre> <p>This code adds a random number to the skewed_col creating a salted_col.</p> <p>While salting is a valuable tool for mitigating data skew, it's crucial to understand your data and assess whether salting is the appropriate solution. In some cases, alternative techniques like broadcast joins or pre-processing the skewed data might be more effective.</p> <p></p> <h2>Cost-Based Optimizer (CBO)</h2> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg" width="2048" height="827" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:827,&quot;width&quot;:2048,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:450602,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12be1da-b217-4b08-a54c-842d86a34d14_2048x827.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p>The Cost-Based Optimizer (CBO) is a technique in Spark SQL that enhances query performance. It uses statistical data to estimate the cost of different query execution plans and selects the most efficient one.</p> <p>To leverage CBO, you first need to collect table and column statistics using the <code>ANALYZE TABLE</code> command. This provides Spark with information about data distribution, cardinality, and other factors. CBO is disabled by default, but you can enable it by setting the following configuration in your app:</p> <pre><code><code>spark.sql.cbo.enabled = true</code></code></pre> <p>By analyzing these statistics, CBO helps Spark make informed decisions about how to execute SQL queries, leading to significant performance improvements, especially for complex queries and large datasets. This translates to faster query processing and reduced resource consumption.</p> <p></p> <h2>Checkpointing</h2> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg" width="1704" height="1383" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1383,&quot;width&quot;:1704,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:307553,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F246a4784-f33d-412a-a111-131a41b20477_1704x1383.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p></p> <p>In ETL processes, we often use caching or persisting when performing multiple actions on the same DataFrame. The <code>cache()</code> method, by default, stores the DataFrame in memory. The <code>persist()</code> method offers more flexibility, allowing you to choose a specific storage level (e.g., memory-only, disk-only, or a combination).</p> <p>However, checkpointing works differently. Instead of storing the DataFrame in memory, it saves it to a persistent storage system like HDFS or S3. A key difference is that checkpointing truncates the DataFrame's lineage.</p> <p>While checkpointing might seem inefficient at first glance&#8212;retrieving data from disk is slower than accessing it from memory, and the lineage information is lost&#8212;it offers significant advantages in certain scenarios.</p> <p>Checkpointing is particularly useful in stateful transformations, where data is processed over multiple batches. In these transformations, a long chain of dependencies between RDDs (Resilient Distributed Datasets) can develop. This makes recovery from failures time-consuming, as Spark needs to retrace the entire lineage to rebuild the data.</p> <p>Checkpointing helps by breaking this dependency chain. By saving RDDs to persistent storage at specific points, it provides a recovery point. In case of an error, Spark can restart from the last checkpoint, significantly reducing recovery time and improving fault tolerance.</p> <p>This approach is especially valuable for large datasets and complex computations, where the overhead of saving intermediate data to disk is outweighed by the potential cost of re-computing everything in case of a failure.</p> <h2>Conclusion</h2> <p>This is not an exhaustive list; many options exist for Spark optimization, ranging from simple "low-hanging fruit" to complex fine-tuning. However, as you progress from basic to advanced techniques, you'll encounter diminishing returns. At some point, it becomes impractical to invest further time and resources into optimizing an already highly optimized system.</p> <p></p> <p><strong>A note about the images:</strong> The visuals in this article were created with the assistance of Imagen 3, Google's text-to-image AI model.</p> <div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://shubhamgondane.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading The Data Engineering Newsletter! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"/><input type="submit" class="button primary" value="Subscribe"/><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Take your Spark pipelines to the next level! Dive into advanced optimization techniques for faster processing and reduced costs.]]></summary></entry><entry><title type="html">Data Engineering: Cost Optimization Part 1</title><link href="https://shubhamgondane.github.io/blog/2024/data-engineering-cost-optimization-part-1/" rel="alternate" type="text/html" title="Data Engineering: Cost Optimization Part 1"/><published>2024-08-02T03:53:49+00:00</published><updated>2024-08-02T03:53:49+00:00</updated><id>https://shubhamgondane.github.io/blog/2024/data-engineering-cost-optimization-part-1</id><content type="html" xml:base="https://shubhamgondane.github.io/blog/2024/data-engineering-cost-optimization-part-1/"><![CDATA[<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic" width="1034" height="366" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:366,&quot;width&quot;:1034,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:13789,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p>Apache Spark is a distributed processing system used for big data workloads, suitable for data engineering, machine learning, and data science use cases. As data engineers, we often manage hundreds of pipelines that process terabytes and petabytes of data. When developing a new ETL for a business case, the focus is typically not on the cost of running the pipelines. ETL optimization and cost reduction often come as an afterthought, usually prompted by reviewing the cloud cost bill.</p> <p>Here are 4 ways that will help you reduce your ETL cost.</p> <div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://shubhamgondane.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading The Data Engineering Newsletter! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"/><input type="submit" class="button primary" value="Subscribe"/><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div> <ul><li><p><strong>Reduce ETL run frequency</strong></p></li><li><p><strong>Process only what you need</strong></p></li><li><p><strong>Tune your Spark config</strong></p></li><li><p><strong>Tune your transformer/queries</strong></p></li></ul> <p>Let&#8217;s cover each of these ideas in depth below.</p> <h4>1. Reduce ETL run frequency</h4> <p>One simple way to cut ETL costs is to assess if a pipeline is truly necessary. At times, we may develop a solution, only to discover months later that no one uses it. In such instances, we can just stop the pipeline.</p> <p>However, what if someone does use the report generated by your pipeline? The crucial question is how often they access the report. Perhaps you refresh the data everyday, but the user only views the report monthly. In this scenario, you could reduce the ETL frequency to a monthly run.</p> <p>A long-term solution involves considering this during the initial design stages. It's often sensible to ask the business whether they genuinely need the dashboard to be refreshed in real-time, hourly, or daily, and to push back when it doesn't seem necessary. See this amazing post by SeattleDataGuy where he explains how to <a href="https://seattledataguy.substack.com/p/understanding-business-needs-staying">understand business needs</a> in more detail.</p> <div><hr/></div> <h4>2. Process only the what you need</h4> <p>There are two methods to process your data efficiently:</p> <ul><li><p>Only process the necessary records.</p></li><li><p>Only read what you need.</p></li></ul> <p>The first method is straightforward: only process data that is essential for your use case. Often, we end up processing a fixed 30 or 60 days of data on each run, which can be wasteful if the data is static and never updated. However, sometimes it's necessary to read historical data for to calculate certain metrics, which can also be optimized.</p> <p>For instance, we once had a pipeline that processed 6-7 years of data to calculate a few metrics. This pipeline ran every hour, making it quite costly. I redesigned the pipeline to process only the changes that occurred over the previous day. I did a one-time bootstrap, reading the last 7 years of data and calculated the metrics. Afterward, each run of the pipeline only updated the metrics with the changes from the previous day. This redesign cut the cost of that pipeline by over 65% and reduced the run time by 50%, improving our SLA (Service Level Agreement).</p> <p>The second method involves selecting only the necessary columns for your job, especially when your data is stored in a columnar format like Parquet. Parquet offers significant advantages when running analytical queries. By reading only the necessary columns, we minimize time spent on I/O operations and speed up query processing.</p> <p>Along with selecting necessary columns, we should also filter the data that we process. By filtering we can leverage Spark&#8217;s predicate pushdown to minimize the amount of data read.</p> <div><hr/></div> <h4>3. Tune your spark config</h4> <p>Another important aspect when it comes to reducing your pipeline is to understand how much resources does your job need. So a good place to start is to check if your job is underutilizing the Spark cluster.</p> <p>Understanding Spark's resource consumption is crucial. Here are key parameters:</p> <ul><li><p><strong>spark.executor.memory</strong>: Defines the amount of memory available to each executor</p></li><li><p><strong>spark.executor.instances</strong>: Determines the number of executors to run the Spark job</p></li><li><p><strong>spark.executor.cores</strong>: Specifies the number of cores per executor, which dictates the number of tasks an executor can run simultaneously</p></li><li><p><strong>spark.driver.memory</strong>: Determines the amount of memory available to the driver</p></li><li><p><strong>spark.driver.cores</strong>: Sets the number of cores available for the driver</p></li><li><p><strong>spark.sql.shuffle.partitions</strong>: Dictates the number of partitions to use when shuffling data (used in joins, aggregations, repartitions)</p></li><li><p><strong>spark.sql.default.parallelism</strong>: Sets the default number of RDDs returned by transformations like joins and aggregations</p></li></ul> <p>Example:</p> <pre><code>The first step is to decide on the number of executor cores, which means determining how many tasks an executor can run parallely. Make sure your applications fully utilize the number of cores available on your cluster node. For optimal I/O throughput, it's generally recommended to keep this number at 5.

After you've decided on the number of executor cores per node, the next step is to determine the number of executors per node. Let's assume you're running your application on a 4 node cluster. Assume each node has 16 cores and 104GB of memory.

If we finalize the executor cores at 5 and allocate 1 for Yarn processing, we're left with 15 cores per node. Since we have 4 nodes, the total number of cores is 60.

The total available executors then equals 60/5 = 12.

After designating 1 for the application manager, we're left with 11.

The number of executors per node is thus 12/4 = 3.

The memory per executor equals 104/3 = 34GB.

After accounting for a 10% overhead, the memory per executor is 34*0.9 = 30GB.</code></pre> <p>You can set both spark.driver.memory and spark.driver.cores to the same values as spark.executor.memory and spark.executor.cores, respectively.</p> <h5>Shuffle partitions</h5> <p>The default value is 200. The optimal value greatly depends on your dataset size and the types of transformations you run on it. The goal is to strike a balance between parallelism and overhead.</p> <p>Having too few partitions can result in data skew, where some partitions are significantly larger, causing certain tasks to take longer to finish.</p> <p>Conversely, too many partitions can lead to inefficiencies due to having many small tasks. A good starting point is 2-3 times the number of cores in the cluster. You can then experiment and adjust this number based on resource utilization and performance.</p> <h5>Dynamic allocation</h5> <p>If you have multiple jobs running on the same cluster, consider using dynamic allocation. This allows your application to return resources to the cluster when they're not needed and request them again when there's demand.</p> <div><hr/></div> <h4>4. Tune Your Transformer</h4> <p>The transformer is where your data undergoes various operations. Here are some optimizations you can apply:</p> <ul><li><p>Use dropDuplicates instead of distinct.</p></li><li><p>Minimize the use of user-defined functions (UDFs) and only use them when a function is not available in Spark SQL. UDFs often operate one row at a time, leading to high serialization overhead.</p></li><li><p>Utilize cache() and persist() to store intermediate computations in Spark for reuse in subsequent operations. Remember to unpersist when done!</p></li><li><p>Avoid using count() when exact counts are not necessary.</p></li><li><p>Do not use show() in production code.</p></li><li><p>Implement broadcast joins when one table is significantly smaller than the other. Spark 3 manages this automatically up to a max size of 8GB. This can be configured by setting the spark.sql.autoBroadcastJoinThreshold, which is defaulted to 10MB.</p></li><li><p>Use Adaptive Query Execution (AQE) in Spark 3.0 to help tackle performance issues.</p></li><li><p>Avoid repartition when possible as it triggers shuffle.</p></li></ul> <div><hr/></div> <p>In conclusion, optimizing Spark pipelines involves strategic decisions and fine-tuning at various points in your ETL process. From reconsidering the frequency of your ETL runs and processing only the essential data, to tuning your Spark configuration and transformer, each step contributes to more efficient resource usage and cost reduction. It's crucial to understand your data, your resources, and the needs of your business to make the most out of your Spark applications. Remember, a well-optimized Spark application not only saves resources but also improves performance, reliability, and the overall value delivered to your business.</p> <p>In the next part, we'll explore more advanced optimization techniques, such as salting and compression.</p> <p>References:</p> <p><a href="https://spark.apache.org/docs/latest/configuration.html">https://spark.apache.org/docs/latest/configuration.html</a></p> <p><a href="https://joydipnath.medium.com/how-to-determine-executor-core-memory-and-size-for-a-spark-app-19310c60c0f7">https://joydipnath.medium.com/how-to-determine-executor-core-memory-and-size-for-a-spark-app-19310c60c0f7</a></p> <p></p> <div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://shubhamgondane.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading The Data Engineering Newsletter! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"/><input type="submit" class="button primary" value="Subscribe"/><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Learn how to manage ETL costs by optimizing your spark jobs.]]></summary></entry></feed>