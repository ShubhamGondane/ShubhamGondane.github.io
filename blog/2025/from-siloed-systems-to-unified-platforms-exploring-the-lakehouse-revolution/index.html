<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4>The Evolution of Data Platforms and the Rise of the Lakehouse</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SqBEpcLp2tto2t2c.jpeg"><figcaption>Image generated by Gemini</figcaption></figure> <p>The explosion of data in recent years has presented both opportunities and challenges for businesses. While data lakes offered a flexible and scalable solution for storing vast amounts of raw data, they often lacked the data management capabilities and performance of traditional data warehouses. This disconnect created a significant hurdle for organizations seeking to leverage their data for both analytical insights and advanced applications like machine learning. The paper “Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics” introduced a compelling solution: the lakehouse. This architecture aims to combine the best of both worlds, offering the low-cost storage and schema-on-read flexibility of data lakes with the ACID transactions, data versioning, and query optimization of data warehouses. This article summarizes the key concepts presented in the paper, exploring the motivations behind the lakehouse architecture, its core components, and the challenges it addresses. Furthermore, we will examine how current lakehouse systems like Apache Hudi, Apache Iceberg, and Delta Lake are implementing these principles and shaping the future of data management.</p> <h3>History of data warehousing</h3> <p>Before diving into lakehouses, it’s important to understand the history and limitations of previous systems, as these shortcomings motivated the development of lakehouses.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/524/0*2NrCus6UzkoBvKdL.png"><figcaption><strong>First and Second generation of data warehouse </strong>(Zaharia, M.A., Ghodsi, A., Xin, R., &amp; Armbrust, M. (2021). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. <em>Conference on Innovative Data Systems Research</em>.)</figcaption></figure> <h3>First Generation</h3> <p>Early data warehouses typically collected data from operational databases and centralized it for analytical insights. These warehouses used a schema-on-write approach, optimizing data models for downstream business intelligence. However, with the explosion of data generation, these systems encountered several problems. First, their coupled compute and storage meant that enterprises paid increasingly more for both as data volumes grew. Second, the rising data volume wasn’t just tabular data; it also included unstructured formats like audio, video, text, and images, which traditional warehouses couldn’t handle.</p> <h3>Second Generation</h3> <p>To address these issues, the second generation of data systems emerged, where enterprises dumped all raw data into a data lake. This data resided in generic, open file formats like Parquet and ORC, primarily supporting a schema-on-read approach. A pipeline would extract a subset of data from the lake and load it into a data warehouse for critical decision support and business intelligence applications. Data lakes were typically stored on cloud platforms like AWS S3, Azure Data Lake Storage (ADLS), and Google Cloud Storage (GCS), enabling the separation of storage and compute. Raw data resided in these storage systems and was then ETLed (Extract, Transform, Load) into downstream data warehouses such as BigQuery, Redshift, or Snowflake.</p> <p>The data lake and warehouse approach, while addressing some earlier challenges, introduced its own set of problems:</p> <ul> <li> <strong>Reliability:</strong> Ensuring data consistency, accuracy, completeness, and validity over time, regardless of source or transformations, proved difficult. Maintaining consistency between the data lake and the warehouse was challenging due to data quality issues, bugs, and the high cost of engineering required to make data available for business intelligence.</li> <li> <strong>Data Staleness:</strong> Data in the warehouse often became outdated, no longer reflecting the current state of affairs. A significant lag existed between updates to the data lake and corresponding updates to the warehouse.</li> <li> <strong>Limited Support for Advanced Analytics:</strong> Building machine learning systems on top of data warehouses was difficult. Analyzing large datasets with complex tools using traditional warehouse access methods was inefficient. Using raw data directly from the data lake was also problematic because the transformed features created and used in the warehouse were not available.</li> <li> <strong>Total Cost of Ownership:</strong> Enterprises incurred double the storage costs due to data being duplicated in both the data lake and the warehouse.</li> </ul> <p>The lakehouse architecture attempts to address several of the challenges outlined above. However, data quality and reliability remain key concerns for users.</p> <h3>The lakehouse architecture</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/310/0*ojkqfBV2udkwyH92.png"><figcaption><strong>Data Lakehouse</strong> (Zaharia, M.A., Ghodsi, A., Xin, R., &amp; Armbrust, M. (2021). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. <em>Conference on Innovative Data Systems Research</em>.)</figcaption></figure> <p>The authors define a lakehouse as a data management system built on low-cost, directly accessible storage that also provides the management and performance features of traditional analytical database management systems (DBMS), such as ACID transactions, data versioning, auditing, indexing, caching, and query optimization. It combines the best of both worlds: the low-cost, open-format storage of data lakes with the powerful management and optimization capabilities of data warehouses.</p> <p>The first step in implementing a lakehouse is to utilize a low-cost object store like S3 or GCS, using open standard file formats such as Parquet. Next, a metadata layer is implemented on top of the object store. This layer defines which objects belong to a specific table version, enabling features like ACID transactions. Several systems have adopted this lakehouse architecture, including Delta Lake, Apache Iceberg, and Apache Hudi.</p> <h4>Metadata Layer</h4> <p>Data lake storage systems like S3 or HDFS offer only a low-level object store or file system interface. Even simple operations, such as atomically updating a table that spans multiple files, are not supported. This is where a metadata layer becomes crucial. It stores information about which objects belong to a table within a transaction log, enabling ACID properties. The metadata layer also facilitates schema enforcement and allows setting constraints on ingested data.</p> <h4>SQL Performance</h4> <p>Lakehouses employ various techniques to improve SQL query performance:</p> <ul> <li> <strong>Caching:</strong> Lakehouses can store frequently accessed data in a faster storage layer (like memory or SSD) to accelerate retrieval times.</li> <li> <strong>Auxiliary Data:</strong> Storing data statistics, such as min-max values for a column, allows the lakehouse to perform data skipping during reads. Implementing indexes like Bloom filters further speeds up data retrieval.</li> <li> <strong>Data Layout:</strong> Record ordering, such as clustering, helps by grouping frequently accessed records together. All lakehouses support ordering records using individual dimensions or space-filling curves like Z-order and Hilbert curves to improve locality across multiple dimensions.</li> </ul> <p>The authors have demonstrated the effectiveness of these optimizations. The graph below shows the execution time and the total cost for customers under each service’s pricing model.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/696/0*GzKQH5J0J6Z4-bWm.png"><figcaption>Zaharia, M.A., Ghodsi, A., Xin, R., &amp; Armbrust, M. (2021). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. <em>Conference on Innovative Data Systems Research</em>.</figcaption></figure> <h3>Current State of Lakehouse Systems</h3> <p>The authors also discuss future directions and alternative designs, which we will explore below, comparing them to current open-table formats for lakehouses. We’ll focus on Apache Hudi, Apache Iceberg, and Delta Lake.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qe6HlopYsuJf1Cmt.png"></figure> <h4>New Data Lake Storage Format</h4> <p>The authors suggest that a new storage format could provide more flexibility for lakehouse systems, enabling better data layout optimizations, indexing, and compatibility with modern hardware. While current table formats support open file formats like Parquet, ORC, and Avro, the emergence of a new, more optimized format remains a possibility.</p> <h4>Alternative Designs for Metadata Layers</h4> <p>Each lakehouse system employs a unique metadata management strategy:</p> <ul> <li> <strong>Apache Hudi:</strong> Uses a timeline-based approach, storing metadata in a dedicated HFile-backed metadata table with indexes for files, column statistics, Bloom filters, and record locations. This facilitates efficient transaction management and data skipping.</li> <li> <strong>Apache Iceberg:</strong> Employs a three-tier metadata tree structure (metadata.json, manifest lists, and manifests) designed for efficient handling of large-scale tables with numerous partitions and files. This hierarchical structure enables efficient metadata operations.</li> <li> <strong>Delta Lake:</strong> Combines transaction logs (JSON files recording each table change) and Parquet checkpoints (periodic snapshots of the table state) for metadata management. Metadata is stored in the Delta Log, enabling efficient change tracking, time travel, and ACID transactions.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2jlSNFadLS8GbjS3AUMnvQ.png"><figcaption>Metadata Management Comparison</figcaption></figure> <h4>Different strategies for caching, auxiliary data structures, and data layout.</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Z84dazIuaXM3apTZ.png"></figure> <p><strong>Optimizations: Caching, Auxiliary Data Structures, and Data Layout</strong></p> <p>All three systems utilize similar optimization strategies:</p> <ul> <li> <strong>Efficient Metadata Management:</strong> All three systems utilize metadata to avoid full table scans.</li> <li> <strong>Data Skipping:</strong> Metadata and statistics (e.g., column bounds, Bloom filters) are used to skip irrelevant data files during query execution.</li> <li> <strong>Indexing:</strong> While not always explicit, metadata structures act as indexes, enabling efficient data lookups and filtering.</li> <li> <strong>Caching:</strong> Frequently accessed metadata is cached to minimize I/O operations and improve query performance.</li> </ul> <p>While the core principles are similar, each system implements these optimizations differently. For example, Iceberg emphasizes scan planning, metadata filtering, and predicate pushdown. Hudi focuses on secondary indexing, a dedicated metadata table, and incremental queries. Delta Lake leverages transaction logs and checkpoints for efficient metadata management and data skipping.</p> <p>In summary, Apache Iceberg, Apache Hudi, and Delta Lake are open-source table formats designed to enhance data lake performance and reliability through efficient metadata management, data skipping, indexing, and caching. Each system has its own implementation details and strengths.</p> <h4>ML integrations</h4> <p>All three lakehouse formats (Hudi, Iceberg, and Delta Lake) support time travel and snapshot data access, allowing engineers to retrieve historical data versions. This is valuable for auditing models or retraining them on past data states.</p> <p>Each format also integrates with various machine learning tools:</p> <ul> <li> <strong>Hudi:</strong> Works with Spark-based ML frameworks and integrates with AWS Glue and Amazon Athena.</li> <li> <strong>Iceberg:</strong> Integrates with AWS Glue and is supported by Amazon SageMaker Feature Store, enabling faster query performance for building training datasets.</li> <li> <strong>Delta Lake:</strong> Excels at unifying batch and streaming data processing for end-to-end ML workflows. Its tight integration with MLflow provides experiment tracking, model versioning, and reproducibility, simplifying the management of the entire ML lifecycle from data preparation to deployment.</li> </ul> <h3>Conclusion</h3> <p>The lakehouse architecture represents a significant step forward in the evolution of data platforms. By unifying data warehousing and advanced analytics, it empowers organizations to unlock the full potential of their data. The paper “Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics” laid the foundation for this new paradigm, highlighting the key challenges and opportunities. As we’ve seen, open-source projects like Apache Hudi, Apache Iceberg, and Delta Lake are actively building upon these concepts, each with its own strengths and approaches. While challenges like data quality and governance still persist, the lakehouse is rapidly becoming the preferred architecture for organizations seeking to build a modern, scalable, and efficient data platform. The ability to seamlessly integrate diverse data types, perform complex analytics, and support advanced machine learning workloads within a single system promises to accelerate innovation and drive data-driven decision-making across industries. As the lakehouse ecosystem continues to mature, we can expect even greater advancements in performance, security, and ease of use, further solidifying its position as the future of data management.</p> <h3>References</h3> <ul> <li>Zaharia, M.A., Ghodsi, A., Xin, R., &amp; Armbrust, M. (2021). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. <em>Conference on Innovative Data Systems Research</em>.</li> <li><a href="https://www.databricks.com/research/lakehouse-a-new-generation-of-open-platforms-that-unify-data-warehousing-and-advanced-analytics" rel="external nofollow noopener" target="_blank">https://www.databricks.com/research/lakehouse-a-new-generation-of-open-platforms-that-unify-data-warehousing-and-advanced-analytics</a></li> <li><a href="https://hudi.apache.org/docs/overview" rel="external nofollow noopener" target="_blank">https://hudi.apache.org/docs/overview</a></li> <li><a href="https://iceberg.apache.org/docs/nightly/" rel="external nofollow noopener" target="_blank">https://iceberg.apache.org/docs/nightly/</a></li> <li><a href="https://delta.io" rel="external nofollow noopener" target="_blank">https://delta.io</a></li> </ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7b590042e3fc" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://blog.det.life/from-siloed-systems-to-unified-platforms-exploring-the-lakehouse-revolution-7b590042e3fc" rel="external nofollow noopener" target="_blank">From Siloed Systems to Unified Platforms: Exploring the Lakehouse Revolution</a> was originally published in <a href="https://blog.det.life" rel="external nofollow noopener" target="_blank">Data Engineer Things</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>