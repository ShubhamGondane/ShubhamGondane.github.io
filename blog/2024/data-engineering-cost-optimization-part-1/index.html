<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic" width="1034" height="366" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":366,"width":1034,"resizeWidth":null,"bytes":13789,"alt":"","title":null,"type":"image/heic","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false}' class="sizing-normal" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F771fa1a4-ad48-4456-9391-c86328dcb585_1034x366.heic 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a></figure></div> <p>Apache Spark is a distributed processing system used for big data workloads, suitable for data engineering, machine learning, and data science use cases. As data engineers, we often manage hundreds of pipelines that process terabytes and petabytes of data. When developing a new ETL for a business case, the focus is typically not on the cost of running the pipelines. ETL optimization and cost reduction often come as an afterthought, usually prompted by reviewing the cloud cost bill.</p> <p>Here are 4 ways that will help you reduce your ETL cost.</p> <div class="subscription-widget-wrap-editor" data-attrs='{"url":"https://shubhamgondane.substack.com/subscribe?","text":"Subscribe","language":"en"}' data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"> <div class="preamble"><p class="cta-caption">Thanks for reading The Data Engineering Newsletter! Subscribe for free to receive new posts and support my work.</p></div> <form class="subscription-widget-subscribe"> <input type="email" class="email-input" name="email" placeholder="Type your email…" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"> <div class="fake-input"></div> <div class="fake-button"></div> </div> </form> </div></div> <ul> <li><p><strong>Reduce ETL run frequency</strong></p></li> <li><p><strong>Process only what you need</strong></p></li> <li><p><strong>Tune your Spark config</strong></p></li> <li><p><strong>Tune your transformer/queries</strong></p></li> </ul> <p>Let’s cover each of these ideas in depth below.</p> <h4>1. Reduce ETL run frequency</h4> <p>One simple way to cut ETL costs is to assess if a pipeline is truly necessary. At times, we may develop a solution, only to discover months later that no one uses it. In such instances, we can just stop the pipeline.</p> <p>However, what if someone does use the report generated by your pipeline? The crucial question is how often they access the report. Perhaps you refresh the data everyday, but the user only views the report monthly. In this scenario, you could reduce the ETL frequency to a monthly run.</p> <p>A long-term solution involves considering this during the initial design stages. It's often sensible to ask the business whether they genuinely need the dashboard to be refreshed in real-time, hourly, or daily, and to push back when it doesn't seem necessary. See this amazing post by SeattleDataGuy where he explains how to <a href="https://seattledataguy.substack.com/p/understanding-business-needs-staying" rel="external nofollow noopener" target="_blank">understand business needs</a> in more detail.</p> <div><hr></div> <h4>2. Process only the what you need</h4> <p>There are two methods to process your data efficiently:</p> <ul> <li><p>Only process the necessary records.</p></li> <li><p>Only read what you need.</p></li> </ul> <p>The first method is straightforward: only process data that is essential for your use case. Often, we end up processing a fixed 30 or 60 days of data on each run, which can be wasteful if the data is static and never updated. However, sometimes it's necessary to read historical data for to calculate certain metrics, which can also be optimized.</p> <p>For instance, we once had a pipeline that processed 6-7 years of data to calculate a few metrics. This pipeline ran every hour, making it quite costly. I redesigned the pipeline to process only the changes that occurred over the previous day. I did a one-time bootstrap, reading the last 7 years of data and calculated the metrics. Afterward, each run of the pipeline only updated the metrics with the changes from the previous day. This redesign cut the cost of that pipeline by over 65% and reduced the run time by 50%, improving our SLA (Service Level Agreement).</p> <p>The second method involves selecting only the necessary columns for your job, especially when your data is stored in a columnar format like Parquet. Parquet offers significant advantages when running analytical queries. By reading only the necessary columns, we minimize time spent on I/O operations and speed up query processing.</p> <p>Along with selecting necessary columns, we should also filter the data that we process. By filtering we can leverage Spark’s predicate pushdown to minimize the amount of data read.</p> <div><hr></div> <h4>3. Tune your spark config</h4> <p>Another important aspect when it comes to reducing your pipeline is to understand how much resources does your job need. So a good place to start is to check if your job is underutilizing the Spark cluster.</p> <p>Understanding Spark's resource consumption is crucial. Here are key parameters:</p> <ul> <li><p><strong>spark.executor.memory</strong>: Defines the amount of memory available to each executor</p></li> <li><p><strong>spark.executor.instances</strong>: Determines the number of executors to run the Spark job</p></li> <li><p><strong>spark.executor.cores</strong>: Specifies the number of cores per executor, which dictates the number of tasks an executor can run simultaneously</p></li> <li><p><strong>spark.driver.memory</strong>: Determines the amount of memory available to the driver</p></li> <li><p><strong>spark.driver.cores</strong>: Sets the number of cores available for the driver</p></li> <li><p><strong>spark.sql.shuffle.partitions</strong>: Dictates the number of partitions to use when shuffling data (used in joins, aggregations, repartitions)</p></li> <li><p><strong>spark.sql.default.parallelism</strong>: Sets the default number of RDDs returned by transformations like joins and aggregations</p></li> </ul> <p>Example:</p> <pre><code>The first step is to decide on the number of executor cores, which means determining how many tasks an executor can run parallely. Make sure your applications fully utilize the number of cores available on your cluster node. For optimal I/O throughput, it's generally recommended to keep this number at 5.

After you've decided on the number of executor cores per node, the next step is to determine the number of executors per node. Let's assume you're running your application on a 4 node cluster. Assume each node has 16 cores and 104GB of memory.

If we finalize the executor cores at 5 and allocate 1 for Yarn processing, we're left with 15 cores per node. Since we have 4 nodes, the total number of cores is 60.

The total available executors then equals 60/5 = 12.

After designating 1 for the application manager, we're left with 11.

The number of executors per node is thus 12/4 = 3.

The memory per executor equals 104/3 = 34GB.

After accounting for a 10% overhead, the memory per executor is 34*0.9 = 30GB.</code></pre> <p>You can set both spark.driver.memory and spark.driver.cores to the same values as spark.executor.memory and spark.executor.cores, respectively.</p> <h5>Shuffle partitions</h5> <p>The default value is 200. The optimal value greatly depends on your dataset size and the types of transformations you run on it. The goal is to strike a balance between parallelism and overhead.</p> <p>Having too few partitions can result in data skew, where some partitions are significantly larger, causing certain tasks to take longer to finish.</p> <p>Conversely, too many partitions can lead to inefficiencies due to having many small tasks. A good starting point is 2-3 times the number of cores in the cluster. You can then experiment and adjust this number based on resource utilization and performance.</p> <h5>Dynamic allocation</h5> <p>If you have multiple jobs running on the same cluster, consider using dynamic allocation. This allows your application to return resources to the cluster when they're not needed and request them again when there's demand.</p> <div><hr></div> <h4>4. Tune Your Transformer</h4> <p>The transformer is where your data undergoes various operations. Here are some optimizations you can apply:</p> <ul> <li><p>Use dropDuplicates instead of distinct.</p></li> <li><p>Minimize the use of user-defined functions (UDFs) and only use them when a function is not available in Spark SQL. UDFs often operate one row at a time, leading to high serialization overhead.</p></li> <li><p>Utilize cache() and persist() to store intermediate computations in Spark for reuse in subsequent operations. Remember to unpersist when done!</p></li> <li><p>Avoid using count() when exact counts are not necessary.</p></li> <li><p>Do not use show() in production code.</p></li> <li><p>Implement broadcast joins when one table is significantly smaller than the other. Spark 3 manages this automatically up to a max size of 8GB. This can be configured by setting the spark.sql.autoBroadcastJoinThreshold, which is defaulted to 10MB.</p></li> <li><p>Use Adaptive Query Execution (AQE) in Spark 3.0 to help tackle performance issues.</p></li> <li><p>Avoid repartition when possible as it triggers shuffle.</p></li> </ul> <div><hr></div> <p>In conclusion, optimizing Spark pipelines involves strategic decisions and fine-tuning at various points in your ETL process. From reconsidering the frequency of your ETL runs and processing only the essential data, to tuning your Spark configuration and transformer, each step contributes to more efficient resource usage and cost reduction. It's crucial to understand your data, your resources, and the needs of your business to make the most out of your Spark applications. Remember, a well-optimized Spark application not only saves resources but also improves performance, reliability, and the overall value delivered to your business.</p> <p>In the next part, we'll explore more advanced optimization techniques, such as salting and compression.</p> <p>References:</p> <p><a href="https://spark.apache.org/docs/latest/configuration.html" rel="external nofollow noopener" target="_blank">https://spark.apache.org/docs/latest/configuration.html</a></p> <p><a href="https://joydipnath.medium.com/how-to-determine-executor-core-memory-and-size-for-a-spark-app-19310c60c0f7" rel="external nofollow noopener" target="_blank">https://joydipnath.medium.com/how-to-determine-executor-core-memory-and-size-for-a-spark-app-19310c60c0f7</a></p> <p></p> <div class="subscription-widget-wrap-editor" data-attrs='{"url":"https://shubhamgondane.substack.com/subscribe?","text":"Subscribe","language":"en"}' data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"> <div class="preamble"><p class="cta-caption">Thanks for reading The Data Engineering Newsletter! Subscribe for free to receive new posts and support my work.</p></div> <form class="subscription-widget-subscribe"> <input type="email" class="email-input" name="email" placeholder="Type your email…" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"> <div class="fake-input"></div> <div class="fake-button"></div> </div> </form> </div></div> </body></html>